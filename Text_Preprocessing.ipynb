{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87860943",
   "metadata": {},
   "source": [
    "# Text Preprocessing with NLTK\n",
    "\n",
    "### Explore and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c461ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb04e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the essays dataset\n",
    "essays = pd.read_csv('sweep8_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd33da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22146 entries, 0 to 22145\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ID          22146 non-null  object\n",
      " 1   Essay Text  22146 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 346.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Quickly explore the data\n",
    "essays.head()\n",
    "essays.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa732d",
   "metadata": {},
   "source": [
    "Check for duplicates in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d075023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate observations.\n"
     ]
    }
   ],
   "source": [
    "# Identify the column(s) with the observations\n",
    "observation_columns = ['ID', 'Essay Text']  \n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = essays.duplicated(subset=observation_columns)\n",
    "\n",
    "# Determine the result\n",
    "if duplicates.any():\n",
    "    print(\"Duplicate observations found.\")\n",
    "else:\n",
    "    print(\"No duplicate observations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f44e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate observations\n",
    "essays.drop_duplicates(subset=observation_columns, inplace=True)\n",
    "\n",
    "# Save the modified data to a new CSV file\n",
    "essays.to_csv('essays.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e3e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14752 entries, 0 to 14762\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ID          14752 non-null  object\n",
      " 1   Essay Text  14752 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 345.8+ KB\n"
     ]
    }
   ],
   "source": [
    "essays.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafd367",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06d8794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ninabucekova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ninabucekova/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ninabucekova/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b923a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text of essays into string\n",
    "essays[\"Essay Text\"] = essays[\"Essay Text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "572c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write functions for text preprocessing\n",
    "\n",
    "def preprocess_text_column(csv_file, text_column, new_column):\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Preprocess the text in the specified column\n",
    "    df[new_column] = df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "\n",
    "    # Return the preprocessed text\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77315331",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text_column('essays.csv', 'Essay Text', 'Preprocessed Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c6e54f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Essay Text</th>\n",
       "      <th>Preprocessed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N28280Y</td>\n",
       "      <td>I am happily married, we are grand-parents. Ou...</td>\n",
       "      <td>['happily', 'married', 'grandparent', 'two', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N13960Q</td>\n",
       "      <td>I am retired, not living in London, probably i...</td>\n",
       "      <td>['retired', 'living', 'london', 'probably', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23786Z</td>\n",
       "      <td>I imagine I'll still be teaching french at Pri...</td>\n",
       "      <td>['imagine', 'ill', 'still', 'teaching', 'frenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N17606R</td>\n",
       "      <td>I am retired from work. I enjoy leisurely time...</td>\n",
       "      <td>['retired', 'work', 'enjoy', 'leisurely', 'tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N19466F</td>\n",
       "      <td>Retired and moved further away from London, Su...</td>\n",
       "      <td>['retired', 'moved', 'away', 'london', 'sussex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                         Essay Text  \\\n",
       "0  N28280Y  I am happily married, we are grand-parents. Ou...   \n",
       "1  N13960Q  I am retired, not living in London, probably i...   \n",
       "2  N23786Z  I imagine I'll still be teaching french at Pri...   \n",
       "3  N17606R  I am retired from work. I enjoy leisurely time...   \n",
       "4  N19466F  Retired and moved further away from London, Su...   \n",
       "\n",
       "                                   Preprocessed Text  \n",
       "0  ['happily', 'married', 'grandparent', 'two', '...  \n",
       "1  ['retired', 'living', 'london', 'probably', 'n...  \n",
       "2  ['imagine', 'ill', 'still', 'teaching', 'frenc...  \n",
       "3  ['retired', 'work', 'enjoy', 'leisurely', 'tim...  \n",
       "4  ['retired', 'moved', 'away', 'london', 'sussex...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays_prep = pd.read_csv('essays.csv')\n",
    "essays_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb102d6d",
   "metadata": {},
   "source": [
    "### Some Basic Text Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b91f186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform basic text statistics on the 'text_column' column\n",
    "text_essays = nltk.Text(essays_prep['Preprocessed Text'])\n",
    "type(text_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2357711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[]', 18),\n",
       " (\"['comment']\", 15),\n",
       " (\"['idea']\", 13),\n",
       " (\"['dont', 'know']\", 12),\n",
       " (\"['much']\", 10),\n",
       " (\"['hope']\", 6),\n",
       " (\"['change']\", 5),\n",
       " (\"['enjoying', 'life']\", 4),\n",
       " (\"['much', '50']\", 4),\n",
       " (\"['hopefully', 'today']\", 4),\n",
       " (\"['response']\", 4),\n",
       " (\"['dont', 'want', 'think']\", 4),\n",
       " (\"['fortune', 'teller']\", 4),\n",
       " (\"['hopefully']\", 4),\n",
       " (\"['dont', 'think', 'ill', 'make', '60']\", 4),\n",
       " (\"['today']\", 4),\n",
       " (\"['happily', 'married', 'grandparent', 'two', 'daughter', 'happy', 'fulfilling', 'career', 'retirement', 'enjoying', 'self', 'ache', 'pain', 'exercising', 'taking', 'long', 'walk', 'enjoying', 'retirement', 'enjoying', 'hobby', 'staying', 'active']\",\n",
       "  2),\n",
       " (\"['retired', 'living', 'london', 'probably', 'nottingham', 'partner', 'current', 'partner', 'health', 'quite', 'good', 'havent', 'succumbed', 'something', 'acute', 'illness', 'like', 'cancer', 'playing', 'piano', 'guitar', 'pleasure', 'perhaps', 'studying', 'mathematics', 'physic', 'still', 'money', 'worry', 'mainly', 'unfair', 'tax', 'one', 'form', 'another', 'unhappiness', 'persist', 'though', 'anxiety', 'nonachievement', 'doubt', 'subside', 'due', 'age']\",\n",
       "  2),\n",
       " (\"['imagine', 'ill', 'still', 'teaching', 'french', 'primary', 'level', 'continue', 'involved', 'local', 'babtist', 'church', 'may', 'voluntary', 'work', 'ill', 'still', 'married', 'hopefully', 'seeing', 'future', 'grandchild', 'regularly', 'think', 'body', 'become', 'stiffer', 'despite', 'pilate', 'class', 'swimming', 'hope', 'ill', 'still', 'go', 'scuba', 'diving', 'year', 'husband', 'also', 'possibility', 'might', 'emigrated', 'sydney']\",\n",
       "  2),\n",
       " (\"['retired', 'work', 'enjoy', 'leisurely', 'time', 'garden', 'enjoy', 'going', 'swim', 'spending', 'time', 'partner', 'caravan', 'walking', 'dog', 'enjoy', 'life', 'enjoy', 'health', 'life', 'pretty', 'simple']\",\n",
       "  2)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most Frequent words\n",
    "from nltk import FreqDist\n",
    "freq_distribution = FreqDist(text_essays)\n",
    "freq_distribution.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9c48898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'able', 'spend', 'time', 'hobby', 'le', 'time', 'work', '2',\n",
      "'probably', 'starting', 'slow', 'due', 'health', '3', 'interested',\n",
      "'keeping', 'happy', 'spending', 'much', 'time', 'possible', 'wife',\n",
      "'4', 'trying', 'accomplish', 'thing', 'missed', 'bringing', 'child']\n",
      "['wife', 'would', 'health', 'permitting', 'would', 'scratcing',\n",
      "'living', 'work', 'normal', 'outlook', 'life', 'would', 'plot',\n",
      "'land', 'lie', 'equal', 'ha', 'ha', 'nice', 'chit', 'chatting']; ['1',\n",
      "'car', 'racing', 'motor', 'bike', 'animal', '2', 'wish', 'could',\n",
      "'better', 'let', 'hope', '3', 'could', 'better'] ['work', 'parttime',\n",
      "'spend', 'half', 'walking', 'husband', 'long', 'weekend', 'away',\n",
      "'attending', 'course', 'art', 'ot', 'textile', 'interested',\n",
      "'meeting', 'friend', 'visiting', 'daughter', 'home', 'healthy',\n",
      "'age']; ['1', 'changed', 'direction', 'careerjob', 'maybe',\n",
      "'finished', 'work', 'altogether', '2', 'continue',\n",
      "'involvedinterested', 'church', 'religious', 'duty', '3', 'continue',\n",
      "'icemake', 'cake', '4', 'improve', 'fitness', 'join', 'group', '5',\n",
      "'hopefully', 'child', 'married', 'child'] ['would', 'still', 'want',\n",
      "'multinational', 'business', 'probably', 'le', 'frequent', 'travel',\n",
      "'though', 'depending', 'overall', 'strength', 'health', 'otherwise',\n",
      "'would', 'still', 'want', 'business', 'activity', 'though',\n",
      "'intensity', 'depends', 'financial', 'position', 'interest',\n",
      "'depends', 'money', 'eg', 'leisure', 'travel', 'home', 'life',\n",
      "'would', 'revolve', 'around', 'hobby', 'child', 'probably', 'left',\n",
      "'home', 'section', 'difficult', 'imagine', 'see', 'revolving',\n",
      "'around', 'financial', 'security', 'luck', 'continues', 'health']\n"
     ]
    }
   ],
   "source": [
    "# Finding collocations\n",
    "text_essays.collocations(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0a597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
